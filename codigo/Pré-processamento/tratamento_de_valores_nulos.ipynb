{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leitura dos dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_pickle('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_de_mama_es.pickle')\n",
    "df_2 = pd.read_pickle('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_sp.pickle')\n",
    "df_3 = pd.read_pickle('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_rj.pickle')\n",
    "df_4 = pd.read_pickle('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_de_mama_mg.pickle')\n",
    "\n",
    "df = pd.concat([df_1, df_2, df_3, df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dados_serializados = pickle.dumps(df)\n",
    "with open('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_regiao_sudeste.pickle', 'wb') as arquivo:\n",
    "    arquivo.write(dados_serializados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_regiao_sudeste.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP_DTOCOR     93.598492\n",
      "AP_ETNIA      99.996192\n",
      "AQ_CIDINI3    84.621195\n",
      "AQ_DTINI3     80.205780\n",
      "AQ_MED02      96.760626\n",
      "AQ_MED03      99.163834\n",
      "AQ_MED04      99.858787\n",
      "AQ_MED05      99.985849\n",
      "AQ_MED06      99.994236\n",
      "AQ_MED07      99.995044\n",
      "AQ_MED08      99.995087\n",
      "AQ_MED09      99.995087\n",
      "AQ_MED10      99.995120\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calcular o percentual de valores nulos em cada coluna\n",
    "percentual_nulos = df.isnull().mean() * 100\n",
    "\n",
    "# Filtrar as colunas com mais de 80% de valores nulos\n",
    "colunas_80_percentuais_nulos = percentual_nulos[percentual_nulos > 80]\n",
    "\n",
    "# Exibir as colunas com mais de 80% de valores nulos, colunas a serem excluídas\n",
    "print(colunas_80_percentuais_nulos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a quantidade de valores nulos por coluna\n",
    "nulos_por_coluna = df.isnull().sum()\n",
    "\n",
    "# Criar um DataFrame com o nome da coluna e a quantidade de valores nulos\n",
    "nulos_df = pd.DataFrame({\n",
    "    'Nome da Coluna': nulos_por_coluna.index,\n",
    "    'Quantidade de Valores Nulos': nulos_por_coluna.values\n",
    "})\n",
    "\n",
    "# Salvar o DataFrame como um arquivo CSV\n",
    "nulos_df.to_csv('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/CSV de Análise/quantidade_valores_nulos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas: 18564141\n"
     ]
    }
   ],
   "source": [
    "numero_de_linhas = df.shape[0]\n",
    "\n",
    "# Exibir a quantidade de linhas\n",
    "print(f'Quantidade de linhas: {numero_de_linhas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definindo as colunas que você quer pegar do DataFrame  para criar o Datafram de localidade\n",
    "colunas_para_analise_regiao = ['AP_CODUNI', 'AP_PRIPAL', 'AP_VL_AP', 'AP_UFMUN', 'AP_TPUPS', 'AP_TIPPRE', 'AP_MN_IND', 'AP_CNPJCPF', 'AP_CNPJMNT', 'AP_CNSPCN', 'AP_CEPPCN', 'AP_UFDIF', 'AP_MNDIF', 'AP_CATEND','AP_UNISOL', 'AP_DTSOLIC', 'AP_DTAUT']\n",
    "\n",
    "# Criando o DataFrame B com essas colunas\n",
    "df_analise_regiao = df[colunas_para_analise_regiao].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dados_serializados = pickle.dumps(df_analise_regiao)\n",
    "with open('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/estabelecimento_regiao_sudeste_cancer_mama.pickle', 'wb') as arquivo:\n",
    "    arquivo.write(dados_serializados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AP_MVM', 'AP_CONDIC', 'AP_GESTAO', 'AP_CODUNI', 'AP_AUTORIZ', 'AP_CMP',\n",
      "       'AP_PRIPAL', 'AP_VL_AP', 'AP_UFMUN', 'AP_TPUPS', 'AP_TIPPRE',\n",
      "       'AP_MN_IND', 'AP_CNPJCPF', 'AP_CNPJMNT', 'AP_CNSPCN', 'AP_COIDADE',\n",
      "       'AP_NUIDADE', 'AP_SEXO', 'AP_RACACOR', 'AP_MUNPCN', 'AP_UFNACIO',\n",
      "       'AP_CEPPCN', 'AP_UFDIF', 'AP_MNDIF', 'AP_DTINIC', 'AP_DTFIM',\n",
      "       'AP_TPATEN', 'AP_TPAPAC', 'AP_MOTSAI', 'AP_OBITO', 'AP_ENCERR',\n",
      "       'AP_PERMAN', 'AP_ALTA', 'AP_TRANSF', 'AP_DTOCOR', 'AP_CODEMI',\n",
      "       'AP_CATEND', 'AP_UNISOL', 'AP_DTSOLIC', 'AP_DTAUT', 'AP_CIDCAS',\n",
      "       'AP_CIDPRI', 'AP_CIDSEC', 'AP_ETNIA', 'AQ_CID10', 'AQ_LINFIN',\n",
      "       'AQ_ESTADI', 'AQ_GRAHIS', 'AQ_DTIDEN', 'AQ_TRANTE', 'AQ_CIDINI1',\n",
      "       'AQ_DTINI1', 'AQ_CIDINI2', 'AQ_DTINI2', 'AQ_CIDINI3', 'AQ_DTINI3',\n",
      "       'AQ_CONTTR', 'AQ_DTINTR', 'AQ_ESQU_P1', 'AQ_TOTMPL', 'AQ_TOTMAU',\n",
      "       'AQ_ESQU_P2', 'AP_NATJUR', 'AQ_MED01', 'AQ_MED02', 'AQ_MED03',\n",
      "       'AQ_MED04', 'AQ_MED05', 'AQ_MED06', 'AQ_MED07', 'AQ_MED08', 'AQ_MED09',\n",
      "       'AQ_MED10', 'CID_PRIN_FORMATADO'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Supondo que você já tenha carregado o DataFrame 'df'\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colunas_para_excluir = ['AP_CODUNI', 'AP_VL_AP', 'AP_TPUPS', 'AP_TIPPRE', 'AP_MN_IND', 'AP_CNPJCPF', 'AP_CNPJMNT', 'AP_CEPPCN', 'AP_UFDIF', 'AP_MNDIF', 'AP_CATEND','AP_UNISOL', 'AP_DTSOLIC', 'AP_DTAUT']\n",
    "\n",
    "# Excluindo as mesmas colunas do dataframe principal\n",
    "df.drop(columns=colunas_para_excluir, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_null_80 = ['AP_CONDIC', 'AP_GESTAO', 'AP_AUTORIZ','AP_CMP','AP_COIDADE','AP_CODEMI','AQ_CID10','AP_DTOCOR', 'AP_ETNIA','CID_PRIN_FORMATADO','AP_NATJUR','AQ_MED01','AQ_MED02', 'AQ_MED03', 'AQ_MED04', 'AQ_MED05', 'AQ_MED06', 'AQ_MED07', 'AQ_MED08', 'AQ_MED09',  'AQ_MED10']\n",
    "\n",
    "# Verificar se as colunas existem no DataFrame antes de tentar removê-las\n",
    "colunas_existentes = [col for col in lista_null_80 if col in df.columns]\n",
    "\n",
    "# Remover as colunas que existem\n",
    "df.drop(colunas_existentes, axis=1, inplace=True)\n",
    "\n",
    "# Exibir o DataFrame após a remoção (opcional)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tratamento de Valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP_PRIPAL         1573\n",
      "AP_CNSPCN        19213\n",
      "AP_UFNACIO          43\n",
      "AQ_ESTADI          271\n",
      "AQ_GRAHIS         9383\n",
      "AQ_CIDINI1     9066379\n",
      "AQ_DTINI1      7798127\n",
      "AQ_CIDINI2    12394286\n",
      "AQ_DTINI2     11345486\n",
      "AQ_CIDINI3    15709198\n",
      "AQ_DTINI3     14889514\n",
      "AQ_ESQU_P1           6\n",
      "AQ_ESQU_P2     5720075\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir as linhas onde a coluna 'AP_CNSPCN' tem valores nulos (NaN)\n",
    "df.dropna(subset=['AP_CNSPCN'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  nan  39.  45. 105. 104.  35.  32. 101.  89.  41. 100.  22.  60.\n",
      " 108.  37. 106.  30.  25.  21.  23.  42. 114. 147. 197. 124. 110.  24.\n",
      " 350. 182. 334. 274. 128. 264. 284. 259. 123.  31. 175. 145. 170. 129.\n",
      "  63.  53.  96.  34. 103. 165. 300.  95. 134. 127. 179. 243. 287. 178.\n",
      " 343. 102. 245.  85.  50. 132.  40. 205. 139. 290. 204. 181. 142.  92.\n",
      "  52.  78.  76.  29. 213. 310. 190.  61.  16. 244. 212.  56. 335. 304.\n",
      "  86. 120. 242.  38. 186.  66. 236. 107. 277. 261. 152. 620.  65.  51.\n",
      "  55.  15.  43. 253. 203. 162. 275. 303. 200. 208. 141. 252. 327. 228.\n",
      "  36. 239. 118. 231. 317.  27. 249. 233. 206. 163. 315. 111. 222. 136.\n",
      " 333. 130. 161. 331. 210.  81. 342. 119. 251. 234. 328. 148. 311. 211.\n",
      " 198.   1.  80.   0. 157. 201. 125. 272. 146. 296. 232.  33.]\n"
     ]
    }
   ],
   "source": [
    "# Verificar os valores únicos da coluna 'AP_UFNACIO' e ordená-los\n",
    "valores_unicos = df['AP_UFNACIO'].unique()\n",
    "\n",
    "\n",
    "# Exibir os valores únicos ordenados\n",
    "print(valores_unicos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dicionário de códigos da Nacionalidade\n",
    "# uf_dict = {\n",
    "#     0: 'Desconhecido', 1: 'Desconhecido', 10: 'Brasil', 'nan': 'Desconhecido', 39: 'Itália', 45: 'Portugal', 105: 'Holanda', \n",
    "#     104: 'Grécia', 35: 'Espanha', 32: 'Grã-Bretanha', 101: 'Ilhas Faroes', 89: 'Peru', 41: 'Japão', 100: 'Escócia', \n",
    "#     22: 'Bolívia', 60: 'Ilhas Guadalupe', 108: 'Ilhas Cosmoledo', 37: 'França', 106: 'Hungria', 30: 'Alemanha', 25: 'Uruguai', \n",
    "#     21: 'Argentina', 23: 'Chile', 42: 'China', 114: 'Iusgoslávia', 147: 'Estônia', 197: 'IFNI', 124:'Romênia', 110:'Inglaterra',  24:'Paraguai',\n",
    "#  350:'Tanzânia', 182:'Ceuta e Melilla', 334: 'Antártica Francesa', 274:'Ruiquiu', 128:'Tchecoeslováquia', 264:'Líbano', 284:'Mianma', 259:'Israel', 123:'Polônia', 31:'Bélgica', 175: 'Angola', 145:'Chuvash', 170:'Abissínia', 129:'Estado da Cidade do Vaticano',\n",
    "#   63:'Honduras Britânica',  53:'Curação',  96:'Bulgária',  34: 'Canadá',  103:'Gibraltar', 165:'Ucrânia', 300:'Ilha Johnston e Sand',  95:'Áustria', 134:'Bósnia Herzegovina', 127:'Svalbard e Jan Mayer', 179:'Botsuana', 243:'Bahrein', 287:'Ashmore e Cartier', 178: 'Benin',\n",
    "#  343: 'Cabo Verde', 102:'Finlândia', 245:'Brunei',  85:'México',  50:'Desconhecido', 132:'Eslovênia',  40:'Comunidade das Bahamas', 205:'Madagascar', 139: 'Bashkista', 290:'Ilhas Cook', 204:'Madawi', 181:'Camarões', 142:'Carelia',  92:'Venezuela',\n",
    "#   52:'Cuba',  78:'São Cristóvão', 242:'Arábia Saudita',  38:'Suíça', 186:'Costa do Marfim',  66:'Jamaica', 236:'Zaire', 107:'Ilhas Baleares', 277:'Síria', 261:'Camboja', 152:'Karachaevocherkess', 620:'Desconhecido',  65:'Ilhas Serranas',  51:'Costa Rica',\n",
    "#   55:'República Dominicana',  15:'Desconhecido',  43:'Coréia', 253:'Hong-Kong', 203:'Madeira',  162:'Tartaria', 275:'Cingapura', 303:'Ilhas Marianas', 200:'Lesoto', 208:'Maurício', 141:'Buryat', 252:'Filipinas', 327:'Sarawak', 228:'Suazilândia',\n",
    "#   36:'Estados Unidos da América', 239:'Zimbábwe', 118:'Mônaco', 231:'Terr Britânico do Oceano índico', 317:'Nova Zelândia',  27: 'Equador', 249: 'China- Taiwan', 233:'Togo', 206:'Mali', 163:'Turcomenistâo', 315:'Ilhas Nova Caledônia', 111:'Irlanda do Norte', 222:'Saara Espanhol', 136:'Eslováquia',\n",
    "#  333:'Antártico Britânico, território', 130:'Croácia', 161:'Tadjiquistão', 331:'Tuvalu', 210:'Moçambique',  81:'Ilhas Virgens Britânicas', 342:'Bangladesh', 119:'Noruega', 251:'Emirados Árabes Unidos', 234:'Tunísia', 328:'Território de Cocos', 148:'Geórgia', 311:'Ilhas Wake', 211:'Nguane',\n",
    "#  198:'Ascesão e Tristão da Cunha',   1:'Desconhecido',  80:'Ilhas Turca',   0:'Desconhecido',  157:'Mari', 201:'Libéria', 125:'San Marino', 272:'Palestina', 146:'Dagesta', 296:'Ilhas do Pacífico', 232:'Transkei',  33:'Aruba',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_dict = {\n",
    "    0: 'Desconhecido', 1: 'Desconhecido', 10: 'Brasil', 'nan': 'Desconhecido', 39: 'Europa', 45: 'Europa', 105: 'Europa',\n",
    "    104: 'Europa', 35: 'Europa', 32: 'Europa', 101: 'Europa', 89: 'América do Sul', 41: 'Ásia', 100: 'Europa',\n",
    "    22: 'América do Sul', 60: 'América do Norte', 108: 'Oceania', 37: 'Europa', 106: 'Europa', 30: 'Europa', 25: 'América do Sul',\n",
    "    21: 'América do Sul', 23: 'América do Sul', 42: 'Ásia', 114: 'Europa', 147: 'Europa', 197: 'África', 124: 'Europa', 110: 'Europa', 24: 'América do Sul',\n",
    "    350: 'África', 182: 'África', 334: 'Antártica', 274: 'Oceania', 128: 'Europa', 264: 'Ásia', 284: 'Ásia', 259: 'Ásia', 123: 'Europa', 31: 'Europa', \n",
    "    175: 'África', 145: 'Ásia', 170: 'África', 129: 'Europa', 63: 'América Central', 53: 'América Central', 96: 'Europa', 34: 'América do Norte', \n",
    "    103: 'Europa', 165: 'Europa', 300: 'Oceania', 95: 'Europa', 134: 'Europa', 127: 'Oceania', 179: 'África', 243: 'Ásia', 287: 'Oceania', 178: 'África',\n",
    "    343: 'África', 102: 'Europa', 245: 'Ásia', 85: 'América do Norte', 50: 'Desconhecido', 132: 'Europa', 40: 'América Central', 205: 'África', 139: 'Ásia', \n",
    "    290: 'Oceania', 204: 'África', 181: 'África', 142: 'Ásia', 92: 'América do Sul', 52: 'América Central', 78: 'América do Norte', 242: 'Ásia', \n",
    "    38: 'Europa', 186: 'África', 66: 'América Central', 236: 'África', 107: 'Europa', 277: 'Ásia', 261: 'Ásia', 152: 'Ásia', 620: 'Desconhecido', \n",
    "    65: 'Oceania', 51: 'América Central', 55: 'América Central', 15: 'Desconhecido', 43: 'Ásia', 253: 'Ásia', 203: 'Europa', 162: 'Ásia', \n",
    "    275: 'Ásia', 303: 'Oceania', 200: 'África', 208: 'África', 141: 'Ásia', 252: 'Ásia', 327: 'Ásia', 228: 'África', 36: 'América do Norte', \n",
    "    239: 'África', 118: 'Europa', 231: 'Oceania', 317: 'Oceania', 27: 'América do Sul', 249: 'Ásia', 233: 'África', 206: 'África', 163: 'Ásia', \n",
    "    315: 'Oceania', 111: 'Europa', 222: 'África', 136: 'Europa', 333: 'Antártica', 130: 'Europa', 161: 'Ásia', 331: 'Oceania', 210: 'África', \n",
    "    81: 'Oceania', 342: 'Ásia', 119: 'Europa', 251: 'Ásia', 234: 'África', 328: 'Oceania', 148: 'Ásia', 311: 'Oceania', 211: 'África', \n",
    "    198: 'Oceania', 1: 'Desconhecido', 80: 'Oceania', 0: 'Desconhecido', 157: 'Ásia', 201: 'África', 125: 'Europa', 272: 'África', 146: 'Ásia', \n",
    "    296: 'Oceania', 232: 'África', 33: 'América Central'\n",
    "}\n",
    "\n",
    "# Substituindo os valores da coluna 'a' pelos continentes correspondentes do dicionário\n",
    "df['AP_UFNACIO'] = df['AP_UFNACIO'].map(uf_dict)\n",
    "\n",
    "\n",
    "# Substituindo NaN por 'Desconhecido' na coluna 'AP_UFNACIO'\n",
    "df['AP_UFNACIO'] = df['AP_UFNACIO'].fillna('Desconhecido')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.04050121e+08 3.04050040e+08 3.04020346e+08 3.04050105e+08\n",
      " 3.04050113e+08 3.04020141e+08 3.04020338e+08 3.04020133e+08\n",
      " 3.04040029e+08 3.04050067e+08 3.04050083e+08 3.04050091e+08\n",
      " 3.04050148e+08 3.04050245e+08 3.04050130e+08 3.04040037e+08\n",
      " 3.04020354e+08 3.04050075e+08 3.04050059e+08 3.04080071e+08\n",
      "            nan 3.04050300e+08 3.04050318e+08 3.04050288e+08\n",
      " 3.04050270e+08 3.04050261e+08 3.04040185e+08 3.04050296e+08\n",
      " 3.04020419e+08 3.04020427e+08 3.04040193e+08 3.04020435e+08\n",
      " 3.04020443e+08 3.04060020e+08 3.04020273e+08 3.04020184e+08\n",
      " 3.04040045e+08 3.04020060e+08 3.04050024e+08 3.04020079e+08\n",
      " 3.04020036e+08 3.04080055e+08 3.04020010e+08 3.04020028e+08\n",
      " 3.04050202e+08 3.04020052e+08 3.04070017e+08 3.04040142e+08\n",
      " 3.04020214e+08 3.04050172e+08 3.04020281e+08 3.04050032e+08\n",
      " 3.04030163e+08 3.04030031e+08 3.04020249e+08 3.04030155e+08\n",
      " 3.04060119e+08 3.04020109e+08 3.04020290e+08 3.04020095e+08\n",
      " 3.04050199e+08 3.04060160e+08 3.04020044e+08 3.04020206e+08\n",
      " 3.04030040e+08 3.04060194e+08 3.04040070e+08 3.04020125e+08\n",
      " 3.04020230e+08 3.04020168e+08 3.04040100e+08 3.04040061e+08\n",
      " 3.04060186e+08 3.04040010e+08 3.04020257e+08 3.04030180e+08\n",
      " 3.04040134e+08 3.04020176e+08 3.04020265e+08 3.04060135e+08\n",
      " 3.04060089e+08 3.04020222e+08 3.04040088e+08 3.04030198e+08\n",
      " 3.04020087e+08 3.04020311e+08 3.04040126e+08 3.04040096e+08\n",
      " 3.04060070e+08 3.04040150e+08]\n"
     ]
    }
   ],
   "source": [
    "# Verificar os valores únicos da coluna\n",
    "valores_unicos = df['AP_PRIPAL'].unique()\n",
    "\n",
    "\n",
    "# Substituindo NaN por 'Desconhecido' na coluna 'AP_PRIPAL'\n",
    "df['AP_PRIPAL'] = df['AP_PRIPAL'].fillna('Desconhecido')\n",
    "\n",
    "# Exibir os valores únicos \n",
    "print(valores_unicos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael\\AppData\\Local\\Temp\\ipykernel_8012\\3193253868.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['AQ_ESTADI'] = df['AQ_ESTADI'].fillna('5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0 1.0 4.0 3.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Verificar os valores únicos da coluna\n",
    "valores_unicos = df['AQ_ESTADI'].unique()\n",
    "\n",
    "\n",
    "# Substituindo NaN por 'Desconhecido' na coluna 'AP_PRIPAL'\n",
    "df['AQ_ESTADI'] = df['AQ_ESTADI'].fillna('5')\n",
    "\n",
    "# Dropar as linhas onde o valor da coluna 'a' é 'Desconhecido'\n",
    "df = df.drop(df[df['AQ_ESTADI'] == '5'].index)\n",
    "\n",
    "#Dropar as linhas onde o valor da coluna 'a' é 'Desconhecido'\n",
    "df = df.drop(df[df['AQ_ESTADI'] == 'nan'].index)\n",
    "\n",
    "\n",
    "# Exibir os valores únicos \n",
    "print(valores_unicos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GX' 'G2' 'G3' 'G1' 'G4' '1' '4' '3' '2' '7' '5' '6' '04' '9' '10' '8'\n",
      " '11' '0' '03' '06' '01' '02' '09' '00' 'II' 'I' 'X' '20' 'Nã' '07' 'SG'\n",
      " 'G' 'GH' 'B2' '2B' 'S' 'X0' '22' '08' '30' '31' '3C' '27' '60' '2A' 'N'\n",
      " 'G6' 'B1' 'XX' 'O' 'G0' 'A' '05' '12' 'B' 'A2' '24' '32' '2S' '21' '3]'\n",
      " 'AL' 'C2' 'G7' '25' 'GL' '1A' 'S0' '3A' '23' '33' '2N' 'Ba' '0,' ',0'\n",
      " '52' 'Al' 'NT' '16' 'C' '26' 'M¿' 'IV' '71' 'CH' '40' '91' '14' nan 'BX'\n",
      " '28' '81' 'OO' 'GI' '13' '4G' '0X' '3G' 'BA' 'GG' '0-' 'X2' 'NI' 'N0'\n",
      " '50' 'X3' '99' 'G8' '0I' '18' '??' '29' '19' '--' '41' '4N' '.' '15' '42'\n",
      " 'GE' '61' 'I0' '-' 'C1' '1S' 'D2' 'T4' '43' ',2' '17' '1N' '2,' 'N1' 'V'\n",
      " ',' '1I' '2G' '62' '1G' '35' 'IA' '3B' 'F1' 'XS' 'X1' '51' '-*' 'N3' 'GT'\n",
      " 'G5' 'X4' 'G9' '.X' '80' '63' 'Z' '2X' 'C5' 'F2' 'GC' '36' ',4' 'CA' '1X'\n",
      " '..' 'T2' 'EC' 'E' '4A' '39' '49' 'g' 'GZ' 'HX' 'AD' '9*' 'BD' 'PD' 'GR'\n",
      " 'ad' 'NE' 'Ad' 'X.' 'EG' 'GJ' 'HE' 'gr' 'MO' 'M' 'AG' '44' 'MD' 'NO' 'T1'\n",
      " 'MA' '90' 'TA' 'SC' 'B3' 'GV' 'DG' 'GN' 'P2' 'VA' 'ID' '3,' 'PT' '1B'\n",
      " '2P' '9G' 'AR' 'H1' 'EI' 'AN' 'CI' 'T3' 'YP' 'TX' '0+' 'P3' 'H3' '2]'\n",
      " 'H2' 'F3' 'C3' '2M' 'LA' 'E1' 'IB' 'LU' 'XG' 'NÒ' '9X' '~G' '7B' '2C'\n",
      " 'ND' 'CD' '3S' '0S' '0\\\\' '0N' '0.' 'gx' '2I' '92' 'S2' 'S1' 'N2' 'J'\n",
      " 'B?' '4B' '45' 'UV' 'GB' 'VB' 'CX' 'FX' '1,' 'GF' 'GA' '2.' 'N/' '3N'\n",
      " 'BG' 'SI' ',,' 'IN' 'I1' 'I2' '+3' '4I' 'NC' 'MX' 'NM' 'MC' 'MN' 'ZX'\n",
      " 'CC' '4C' '3X' 'TG' 'xx' '1]' '4S' 'x' '70' ']X' 'GO' ',G']\n"
     ]
    }
   ],
   "source": [
    "# Verificar os valores únicos da coluna\n",
    "valores_unicos = df['AQ_GRAHIS'].unique()\n",
    "\n",
    "\n",
    "# Exibir os valores únicos \n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo NaN por 'Desconhecido' na coluna\n",
    "df['AQ_GRAHIS'] = df['AQ_GRAHIS'].fillna('Desconhecido')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esquema_terapeutico\n",
       "TAMOXIFENO         3069113\n",
       "ANASTROZOL         2882424\n",
       "TMX                2664543\n",
       "ARIMIDEX            522063\n",
       "TAMOX               375007\n",
       "                    ...   \n",
       "AC+FILGRASTIM+P          1\n",
       "HERCEP PERTU XE          1\n",
       "GEMZAR/H/P/ZOM           1\n",
       "taxocarbo                1\n",
       "CARBO+TAXO,GCSF          1\n",
       "Name: count, Length: 50379, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['esquema_terapeutico'] = df['AQ_ESQU_P1'].fillna('') + df['AQ_ESQU_P2'].fillna('') \n",
    "df['esquema_terapeutico'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluindo as colunas 'AQ_ESQU_P1' e 'AQ_ESQU_P2'\n",
    "\n",
    "df.drop(columns=['AQ_ESQU_P1', 'AQ_ESQU_P2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael\\AppData\\Local\\Temp\\ipykernel_15508\\2206514680.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Tempo_2_3'].fillna(0, inplace=True)\n",
      "C:\\Users\\Rafael\\AppData\\Local\\Temp\\ipykernel_15508\\2206514680.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Tempo_1_2'].fillna(0, inplace=True)\n",
      "C:\\Users\\Rafael\\AppData\\Local\\Temp\\ipykernel_15508\\2206514680.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Tempo_1_3'].fillna(0, inplace=True)\n",
      "C:\\Users\\Rafael\\AppData\\Local\\Temp\\ipykernel_15508\\2206514680.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Tempo_Atual_1'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Certifique-se de que as colunas estão no formato datetime\n",
    "df['AQ_DTINI1'] = pd.to_datetime(df['AQ_DTINI1'], errors='coerce')\n",
    "df['AQ_DTINI2'] = pd.to_datetime(df['AQ_DTINI2'], errors='coerce')\n",
    "df['AQ_DTINI3'] = pd.to_datetime(df['AQ_DTINI3'], errors='coerce')\n",
    "df['AQ_DTINTR'] = pd.to_datetime(df['AQ_DTINTR'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# Calcular a diferença em dias entre AQ_DTINI1 e AQ_DTINI2\n",
    "# df['Tempo_Atual_1'] = (df['AQ_DTINI1'] - df['AQ_DTINTR']).dt.days\n",
    "\n",
    "# # Calcular a diferença em dias entre AQ_DTINI1 e AQ_DTINI2\n",
    "# df['Tempo_1_2'] = (df['AQ_DTINI2'] - df['AQ_DTINI1']).dt.days\n",
    "\n",
    "# # Calcular a diferença em dias entre AQ_DTINI2 e AQ_DTINI3, considerando 0 se AQ_DTINI3 for nulo\n",
    "# df['Tempo_2_3'] = (df['AQ_DTINI3'] - df['AQ_DTINI2']).dt.days\n",
    "\n",
    "# # Calcular a diferença em dias entre AQ_DTINI1 e AQ_DTINI3, considerando 0 se AQ_DTINI2, ou AQ_DTINI3 for nulo\n",
    "# df['Tempo_1_3'] = (df['AQ_DTINI3'] - df['AQ_DTINI1']).dt.days\n",
    "\n",
    "# # Substituir valores nulos na coluna por 0\n",
    "# df['Tempo_2_3'].fillna(0, inplace=True)\n",
    "# df['Tempo_1_2'].fillna(0, inplace=True)\n",
    "# df['Tempo_1_3'].fillna(0, inplace=True)\n",
    "# df['Tempo_Atual_1'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['AQ_DTINI1', 'AQ_DTINI2','AQ_DTINI3'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratar os CID (retirar o último número)\n",
    "df['AQ_CIDINI1'] = df['AQ_CIDINI1'].str.slice(0, -1)\n",
    "df['AQ_CIDINI2'] = df['AQ_CIDINI2'].str.slice(0, -1)\n",
    "df['AQ_CIDINI3'] = df['AQ_CIDINI3'].str.slice(0, -1)\n",
    "df['AP_CIDPRI'] = df['AP_CIDPRI'].str.slice(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Mudanca_de_Diagnostico_1'] = (df['AP_CIDPRI'] != df['AQ_CIDINI1']) & df['AP_CIDPRI'].notna().astype(int)\n",
    "# df['Mudanca_de_Diagnostico_2'] = (df['AQ_CIDINI2'] != df['AQ_CIDINI1']) & df['AQ_CIDINI2'].notna().astype(int)\n",
    "# df['Mudanca_de_Diagnostico_3'] = (df['AQ_CIDINI3'] != df['AQ_CIDINI2']) & df['AQ_CIDINI3'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Numero_de_Tratamentos_Anteriores'] = df[['AQ_CIDINI1', 'AQ_CIDINI2', 'AQ_CIDINI3']].notna().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de diagnósticos diferentes ao longo dos tratamentos\n",
    "df['Numero_de_Diagnosticos_Diferentes'] = df[['AQ_CIDINI1', 'AQ_CIDINI2', 'AQ_CIDINI3', 'AP_CIDPRI']].nunique(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma variável para indicar se há causas associadas ao tratamento\n",
    "df['Tem_Causas_Associadas'] = df['AP_CIDCAS'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma variável para indicar se há causas associadas ao tratamento\n",
    "df['Tem_Causas_Secundaria'] = df['AP_CIDSEC'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['AQ_CIDINI1', 'AQ_CIDINI2','AQ_CIDINI3', 'AP_CIDPRI'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8123\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Filtrando e contando as linhas que têm o valor '{{{{{{{{{{{{{{' na coluna 'AP_CNSPCN'\n",
    "count = df[df['AP_CNSPCN'] == '{{{{{{{{{{{{{{{'].shape[0]\n",
    "\n",
    "print(count)\n",
    "\n",
    "# Remover as linhas que têm o valor '{{{{{{{{{{{{{{' na coluna 'AP_CNSPCN'\n",
    "df = df.drop(df[df['AP_CNSPCN'] == '{{{{{{{{{{{{{{{'].index)\n",
    "\n",
    "\n",
    "\n",
    "count = df[df['AP_CNSPCN'] == '{{{{{{{{{{{{{{{'].shape[0]\n",
    "\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP_PRIPAL', 'AP_UFMUN', 'AP_CNSPCN', 'AP_NUIDADE', 'AP_SEXO', 'AP_RACACOR', 'AP_MUNPCN', 'AP_UFNACIO', 'AP_DTINIC', 'AP_DTFIM', 'AP_MOTSAI', 'AP_OBITO', 'AP_ENCERR', 'AP_PERMAN', 'AP_ALTA', 'AP_TRANSF', 'AP_CIDCAS', 'AP_CIDSEC', 'AQ_LINFIN', 'AQ_ESTADI', 'AQ_GRAHIS', 'AQ_TRANTE', 'AQ_CONTTR', 'esquema_terapeutico', 'CID_principal', 'Numero_de_Tratamentos_Anteriores', 'Numero_de_Diagnosticos_Diferentes', 'Tem_Causas_Associadas', 'Tem_Causas_Secundaria']\n"
     ]
    }
   ],
   "source": [
    "colunas = df.columns.tolist()\n",
    "print(colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ES' 'MG' 'BA' 'RJ' 'SC' 'RO' 'AL' 'SP' 'PR' 'RS' 'CE' 'RR' 'PA' 'MS'\n",
      " 'RN' 'MT' 'MA' 'AC' 'SE' 'DF' 'TO' 'PE' 'PI' 'AM' 'PB' 'AP']\n"
     ]
    }
   ],
   "source": [
    "# Função para limitar a dois primeiros números\n",
    "df['AP_MUNPCN'] = df['AP_MUNPCN'].apply(lambda x: int(str(x)[:2]))\n",
    "df['AP_UFMUN'] = df['AP_UFMUN'].apply(lambda x: int(str(x)[:2]))\n",
    "\n",
    "# Dicionário de códigos da Unidade da Federação para siglas dos estados\n",
    "uf_dict = {\n",
    "    11: 'RO', 12: 'AC', 13: 'AM', 14: 'RR', 15: 'PA', 16: 'AP', 17:'TO',   \n",
    "    21: 'MA', 22: 'PI', 23: 'CE', 24: 'RN', 25:'PB', 26:'PE', 27: 'AL', 28:'SE', 29: 'BA', \n",
    "    31: 'MG', 32: 'ES', 33: 'RJ', 35: 'SP',\n",
    "    41: 'PR',  42: 'SC', 43: 'RS', \n",
    "    50:'MS', 51:'MT', 52: 'RS', 53: 'DF'\n",
    "}\n",
    "\n",
    "# Substituindo os códigos pelos valores de sigla do estado\n",
    "df['AP_MUNPCN'] = df['AP_MUNPCN'].map(uf_dict)\n",
    "df['AP_UFMUN'] = df['AP_UFMUN'].map(uf_dict)\n",
    "\n",
    "\n",
    "valores_unicos = df['AP_MUNPCN'].unique()\n",
    "print(valores_unicos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['AP_MVM', 'AP_TPATEN', 'AP_TPAPAC', 'AQ_DTIDEN', 'AQ_DTINTR', 'AQ_TOTMPL', 'AQ_TOTMAU'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374349\n",
      "6.329654947655797\n",
      "AP_OBITO\n",
      "0    18369586\n",
      "1       23695\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de valores únicos na coluna 'cns_paciente'\n",
    "quantidade_unicos = df['AP_CNSPCN'].nunique()\n",
    "\n",
    "#Quantidade de óbitos\n",
    "df['AP_OBITO'].value_counts()\n",
    "\n",
    "#Porcentagem de mortalidade\n",
    "mortalidade = df['AP_OBITO'].value_counts()[1] / quantidade_unicos\n",
    "mortalidade = mortalidade * 100\n",
    "\n",
    "\n",
    "# Mostrar o resultado\n",
    "print(quantidade_unicos)\n",
    "print(mortalidade)\n",
    "print(df['AP_OBITO'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AP_PRIPAL                             object\n",
       "AP_UFMUN                              object\n",
       "AP_CNSPCN                             object\n",
       "AP_NUIDADE                             int64\n",
       "AP_SEXO                               object\n",
       "AP_RACACOR                             int64\n",
       "AP_MUNPCN                             object\n",
       "AP_UFNACIO                            object\n",
       "AP_DTINIC                              int64\n",
       "AP_DTFIM                               int64\n",
       "AP_MOTSAI                              int64\n",
       "AP_OBITO                               int64\n",
       "AP_ENCERR                              int64\n",
       "AP_PERMAN                              int64\n",
       "AP_ALTA                                int64\n",
       "AP_TRANSF                              int64\n",
       "AP_CIDCAS                             object\n",
       "AP_CIDSEC                             object\n",
       "AQ_LINFIN                             object\n",
       "AQ_ESTADI                            float64\n",
       "AQ_GRAHIS                             object\n",
       "AQ_TRANTE                             object\n",
       "AQ_CONTTR                             object\n",
       "esquema_terapeutico                   object\n",
       "CID_principal                         object\n",
       "Numero_de_Tratamentos_Anteriores       int64\n",
       "Numero_de_Diagnosticos_Diferentes      int64\n",
       "Tem_Causas_Associadas                  int64\n",
       "Tem_Causas_Secundaria                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['AP_PRIPAL'] != 'Desconhecido']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AP_PRIPAL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AP_PRIPAL'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAQ_ESTADI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAQ_ESTADI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP_PRIPAL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAP_PRIPAL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP_DTINIC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP_DTINIC\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP_DTFIM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP_DTFIM\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AP_PRIPAL'"
     ]
    }
   ],
   "source": [
    "df['AQ_ESTADI'] = df['AQ_ESTADI'].astype(int)\n",
    "df['AP_PRIPAL'] = df['AP_PRIPAL'].astype(int)\n",
    "df['AP_DTINIC'] = pd.to_datetime(df['AP_DTINIC'], format='%Y%m%d').dt.strftime('%d/%m/%Y')\n",
    "df['AP_DTFIM'] = pd.to_datetime(df['AP_DTFIM'], format='%Y%m%d').dt.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dados_serializados \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_regiao_sudeste_sem_nulos.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m arquivo:\n\u001b[0;32m      5\u001b[0m     arquivo\u001b[38;5;241m.\u001b[39mwrite(dados_serializados)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dados_serializados = pickle.dumps(df)\n",
    "with open('C:/Users/Rafael/OneDrive/Documentos/TCC Rafaela/Dados/Pickle/cancer_mama_regiao_sudeste_sem_nulos.pickle', 'wb') as arquivo:\n",
    "    arquivo.write(dados_serializados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
